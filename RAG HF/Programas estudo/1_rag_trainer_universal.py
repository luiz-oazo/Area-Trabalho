#!/usr/bin/env python3
"""
PROGRAMA 1: TREINAMENTO RAG UNIVERSAL
Funciona com Wikipedia, blogs, sites de not√≠cias, documenta√ß√£o, etc.
Otimizado para MacBook
"""

import requests
from bs4 import BeautifulSoup
import faiss
import numpy as np
from sentence_transformers import SentenceTransformer
import pickle
import os
import json
from tqdm import tqdm
import sys
import time
from urllib.parse import urlparse

class UniversalRAGTrainer:
    def __init__(self, save_dir="./rag_system_mac"):
        self.save_dir = save_dir

        # üåê CONFIGURE SUAS URLs AQUI - QUALQUER SITE!
        self.articles = [
            # Exemplos de diferentes tipos de sites:

            # Wikipedia (sempre funciona bem)
            "https://en.wikipedia.org/wiki/Artificial_intelligence",
            "https://en.wikipedia.org/wiki/Machine_learning",

            # Sites de not√≠cias (exemplo)
            # "https://www.bbc.com/news/technology-51064369",

            # Blogs t√©cnicos (exemplo)
            # "https://openai.com/blog/chatgpt",

            # Documenta√ß√£o (exemplo)
            # "https://docs.python.org/3/tutorial/introduction.html",

            # Sites educacionais (exemplo)
            # "https://www.khanacademy.org/computing/intro-to-programming",

            # Adicione suas URLs aqui:
            "https://en.wikipedia.org/wiki/Deep_learning",
            "https://en.wikipedia.org/wiki/Neural_network",
            "https://en.wikipedia.org/wiki/Natural_language_processing"
        ]

        # Headers para parecer um navegador real
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.5',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
        }

    def extract_text_universal(self, soup, url):
        """Extrai texto de qualquer tipo de site."""
        try:
            # Remove elementos indesejados comuns
            unwanted_tags = ['script', 'style', 'nav', 'header', 'footer', 
                           'aside', 'advertisement', 'ads', 'sidebar']

            for tag in unwanted_tags:
                for element in soup.find_all(tag):
                    element.decompose()

            # Remove classes/IDs comuns de elementos indesejados
            unwanted_classes = ['advertisement', 'ads', 'sidebar', 'nav', 
                              'header', 'footer', 'menu', 'social', 'share',
                              'comment', 'related', 'popup', 'modal']

            for class_name in unwanted_classes:
                for element in soup.find_all(class_=class_name):
                    element.decompose()
                for element in soup.find_all(id=class_name):
                    element.decompose()

            # Estrat√©gias de extra√ß√£o por prioridade
            text_content = ""

            # 1. Tenta encontrar conte√∫do principal
            main_selectors = [
                'main', 'article', '.content', '.post', '.entry',
                '.article-body', '.post-content', '.entry-content',
                '#content', '#main', '#article'
            ]

            for selector in main_selectors:
                main_content = soup.select_one(selector)
                if main_content:
                    text_content = main_content.get_text(separator='\n', strip=True)
                    if len(text_content) > 500:  # Conte√∫do substancial
                        break

            # 2. Se n√£o encontrou conte√∫do principal, tenta par√°grafos
            if len(text_content) < 500:
                paragraphs = soup.find_all('p')
                text_parts = []
                for p in paragraphs:
                    p_text = p.get_text().strip()
                    if len(p_text) > 30:  # Ignora par√°grafos muito pequenos
                        text_parts.append(p_text)
                text_content = '\n\n'.join(text_parts)

            # 3. Se ainda n√£o tem conte√∫do, tenta divs com texto
            if len(text_content) < 500:
                divs = soup.find_all('div')
                text_parts = []
                for div in divs:
                    # Ignora divs com muitos elementos filhos (provavelmente layout)
                    if len(div.find_all()) < 5:
                        div_text = div.get_text().strip()
                        if len(div_text) > 50:
                            text_parts.append(div_text)
                text_content = '\n\n'.join(text_parts[:20])  # Limita a 20 divs

            # 4. √öltimo recurso: todo o texto da p√°gina
            if len(text_content) < 200:
                text_content = soup.get_text(separator='\n', strip=True)

            # Limpa o texto
            lines = text_content.split('\n')
            clean_lines = []
            for line in lines:
                line = line.strip()
                if len(line) > 10 and not line.startswith(('¬©', 'Cookie', 'Privacy')):
                    clean_lines.append(line)

            final_text = '\n\n'.join(clean_lines)

            return final_text[:50000]  # Limita a 50k caracteres

        except Exception as e:
            print(f"    ‚ö†Ô∏è Erro na extra√ß√£o de texto: {e}")
            return ""

    def get_article_content_universal(self, url):
        """Extrai conte√∫do de qualquer site."""
        try:
            domain = urlparse(url).netloc
            print(f"  üìÑ Baixando: {domain}")

            # Faz requisi√ß√£o com headers de navegador
            response = requests.get(url, headers=self.headers, timeout=20)
            response.raise_for_status()

            # Detecta encoding
            response.encoding = response.apparent_encoding

            soup = BeautifulSoup(response.content, 'html.parser')

            # Extrai texto usando m√©todo universal
            text = self.extract_text_universal(soup, url)

            if len(text) < 100:
                print(f"    ‚ö†Ô∏è Pouco conte√∫do extra√≠do de {domain}")
                return None

            # Tenta extrair t√≠tulo
            title = ""
            title_selectors = ['h1', 'title', '.title', '.headline', '.post-title']
            for selector in title_selectors:
                title_elem = soup.select_one(selector)
                if title_elem:
                    title = title_elem.get_text().strip()[:100]
                    break

            if not title:
                title = domain

            print(f"    ‚úÖ Extra√≠do: {len(text)} caracteres")

            return {
                'title': title,
                'url': url,
                'domain': domain,
                'content': text.strip()
            }

        except requests.exceptions.RequestException as e:
            print(f"    ‚ùå Erro de conex√£o com {url}: {e}")
            return None
        except Exception as e:
            print(f"    ‚ùå Erro ao processar {url}: {e}")
            return None

    def create_chunks(self, documents):
        """Divide documentos em chunks menores."""
        print("\nüìä Criando chunks de texto...")
        chunks = []
        metadata = []

        for doc_id, doc in enumerate(documents):
            content = doc['content']
            chunk_size = 500  # Chunks maiores para sites diversos
            overlap = 100

            # Divide em chunks com overlap
            for i in range(0, len(content), chunk_size - overlap):
                chunk = content[i:i + chunk_size].strip()
                if len(chunk) > 150:  # Chunks m√≠nimos maiores
                    chunks.append(chunk)
                    metadata.append({
                        'doc_id': doc_id,
                        'title': doc['title'],
                        'url': doc['url'],
                        'domain': doc['domain'],
                        'chunk_id': len(chunks) - 1,
                        'start_pos': i
                    })

        print(f"  ‚úÖ Criados {len(chunks)} chunks de {len(documents)} documentos")
        return chunks, metadata

    def create_embeddings(self, chunks):
        """Cria embeddings dos chunks usando SentenceTransformer."""
        print("\nüîó Gerando embeddings...")

        # Configura√ß√µes para MacBook
        os.environ["TOKENIZERS_PARALLELISM"] = "false"

        try:
            # Modelo otimizado para CPU
            model = SentenceTransformer('all-MiniLM-L6-v2')
            print(f"  üìê Modelo carregado: all-MiniLM-L6-v2")

            # Gera embeddings em lotes pequenos
            batch_size = 16  # Menor para sites diversos
            embeddings = []

            for i in tqdm(range(0, len(chunks), batch_size), 
                         desc="  Processando", unit="lote"):
                batch = chunks[i:i + batch_size]
                batch_emb = model.encode(
                    batch,
                    batch_size=8,
                    show_progress_bar=False,
                    convert_to_numpy=True,
                    normalize_embeddings=True
                )
                embeddings.append(batch_emb)

            # Concatena todos os embeddings
            all_embeddings = np.vstack(embeddings).astype('float32')
            print(f"  ‚úÖ Embeddings criados: {all_embeddings.shape}")

            return model, all_embeddings

        except Exception as e:
            print(f"  ‚ùå Erro ao criar embeddings: {e}")
            return None, None

    def create_faiss_index(self, embeddings):
        """Cria √≠ndice Faiss para busca r√°pida."""
        print("\nüîç Construindo √≠ndice Faiss...")
        try:
            dimension = embeddings.shape[1]
            index = faiss.IndexFlatL2(dimension)
            index.add(embeddings)
            print(f"  ‚úÖ √çndice criado: {index.ntotal} vetores, dimens√£o {dimension}")
            return index
        except Exception as e:
            print(f"  ‚ùå Erro ao criar √≠ndice: {e}")
            return None

    def save_system(self, model, index, chunks, metadata, documents):
        """Salva todo o sistema RAG."""
        print(f"\nüíæ Salvando sistema em: {os.path.abspath(self.save_dir)}")

        try:
            os.makedirs(self.save_dir, exist_ok=True)

            # Salva modelo de embedding
            print("  üì¶ Salvando modelo de embedding...")
            model.save(f"{self.save_dir}/embedding_model")

            # Salva √≠ndice Faiss
            print("  üì¶ Salvando √≠ndice Faiss...")
            faiss.write_index(index, f"{self.save_dir}/faiss_index.bin")

            # Salva dados
            print("  üì¶ Salvando dados...")
            data = {
                'chunks': chunks,
                'metadata': metadata,
                'documents': documents,
                'stats': {
                    'num_chunks': len(chunks),
                    'num_documents': len(documents),
                    'embedding_dim': index.d,
                    'domains': list(set(doc['domain'] for doc in documents))
                }
            }

            with open(f"{self.save_dir}/rag_data.pkl", "wb") as f:
                pickle.dump(data, f)

            # Salva informa√ß√µes do sistema
            info = {
                'created_at': '2024-01-01',
                'model_name': 'all-MiniLM-L6-v2',
                'num_chunks': len(chunks),
                'num_documents': len(documents),
                'embedding_dimension': index.d,
                'domains': [doc['domain'] for doc in documents],
                'urls': [doc['url'] for doc in documents],
                'system_ready': True,
                'type': 'universal'
            }

            with open(f"{self.save_dir}/system_info.json", "w") as f:
                json.dump(info, f, indent=2)

            print("  ‚úÖ Sistema salvo com sucesso!")

            # Mostra estat√≠sticas
            print("\nüìä ESTAT√çSTICAS:")
            domains = [doc['domain'] for doc in documents]
            unique_domains = set(domains)
            print(f"  üåê Dom√≠nios processados: {len(unique_domains)}")
            for domain in unique_domains:
                count = domains.count(domain)
                print(f"    - {domain}: {count} p√°gina(s)")

            return True

        except Exception as e:
            print(f"  ‚ùå Erro ao salvar: {e}")
            return False

    def train(self):
        """Executa o treinamento completo."""
        print("üöÄ INICIANDO TREINAMENTO RAG UNIVERSAL")
        print("=" * 60)
        print("üì± Otimizado para MacBook")
        print("üåê Funciona com qualquer site!")
        print("=" * 60)

        # 1. Coleta documentos
        print("\nüìö COLETANDO DOCUMENTOS...")
        documents = []
        failed_urls = []

        for i, url in enumerate(self.articles, 1):
            print(f"\n[{i}/{len(self.articles)}] Processando...")
            doc = self.get_article_content_universal(url)
            if doc:
                documents.append(doc)
            else:
                failed_urls.append(url)

            # Pausa entre requisi√ß√µes para ser respeitoso
            if i < len(self.articles):
                time.sleep(1)

        if not documents:
            print("‚ùå Nenhum documento coletado!")
            if failed_urls:
                print("\nüö´ URLs que falharam:")
                for url in failed_urls:
                    print(f"  - {url}")
            return False

        print(f"\n‚úÖ {len(documents)} documentos coletados com sucesso")
        if failed_urls:
            print(f"‚ö†Ô∏è {len(failed_urls)} URLs falharam")

        # 2. Cria chunks
        chunks, metadata = self.create_chunks(documents)
        if not chunks:
            print("‚ùå Nenhum chunk criado!")
            return False

        # 3. Cria embeddings
        model, embeddings = self.create_embeddings(chunks)
        if model is None or embeddings is None:
            print("‚ùå Falha ao criar embeddings!")
            return False

        # 4. Cria √≠ndice
        index = self.create_faiss_index(embeddings)
        if index is None:
            print("‚ùå Falha ao criar √≠ndice!")
            return False

        # 5. Salva sistema
        if self.save_system(model, index, chunks, metadata, documents):
            print("\nüéâ TREINAMENTO UNIVERSAL CONCLU√çDO!")
            print("=" * 60)
            print("‚úÖ Sistema RAG pronto para qualquer conte√∫do")
            print("üöÄ Execute agora: python 2_rag_query_smart.py")
            print("üéØ Ou execute: python 3_rag_feedback_memory.py")
            return True
        else:
            print("‚ùå Falha ao salvar sistema!")
            return False

def main():
    """Fun√ß√£o principal."""
    print("üçé RAG TRAINER UNIVERSAL PARA MACBOOK")
    print("Funciona com Wikipedia, blogs, not√≠cias, documenta√ß√£o, etc.\n")

    try:
        trainer = UniversalRAGTrainer()

        print("üåê URLs configuradas:")
        for i, url in enumerate(trainer.articles, 1):
            domain = urlparse(url).netloc
            print(f"  {i}. {domain}")

        print("\nüí° Para usar outros sites, edite a lista 'self.articles' no c√≥digo")
        print("Pressione Ctrl+C para cancelar\n")

        success = trainer.train()

        if success:
            print("\n‚ú® Pronto! Agora voc√™ pode usar os outros programas.")
        else:
            print("\n‚ùå Treinamento falhou. Verifique as URLs e sua conex√£o.")

    except KeyboardInterrupt:
        print("\n\n‚èπÔ∏è  Treinamento cancelado pelo usu√°rio.")
        sys.exit(0)
    except Exception as e:
        print(f"\n‚ùå Erro inesperado: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()
